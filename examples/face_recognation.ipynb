{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\initi\\.conda\\envs\\facenet\\Lib\\site-packages\\tqdm-4.67.0-py3.12.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1,fixed_image_standardization, training, extract_face\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # change backend enviroment so plot is working \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\initi\\AppData\\Roaming\\Python\\Python312\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\initi\\AppData\\Roaming\\Python\\Python312\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\initi\\AppData\\Roaming\\Python\\Python312\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\initi\\AppData\\Roaming\\Python\\Python312\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 of people has been identified\n"
     ]
    }
   ],
   "source": [
    "workers = 0 if os.name == 'nt' else 4\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device,\n",
    "    keep_all=True, # by setting true, retured img_cropped dimension will be batch_size x identified_people_num x chanels x pixel_x x pixel_y\n",
    ")\n",
    "\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "resnet.classify = False # If set to be true, the final fully connected layer will be aciviated, the output dimension is depending on the # of classes that used for the trainning. Adding more people to the trainning will change this number\n",
    "\n",
    "\n",
    "img = Image.open('C:/Users/initi/OneDrive/Documents/GitHub/facenet-pytorch/data/test_images/test/sombody.jpg')  # Replace with the path to your image file\n",
    "img_cropped = mtcnn(img, save_path=\"C:/Users/initi/OneDrive/Documents/GitHub/facenet-pytorch/data/test_images/test/sombody_cropped.jpg\")\n",
    "\n",
    "# box dim : number_of_people x 4 point coordinates (x_min, y_min, x_max, y_max)\n",
    "# points dim : number_of_people x 5 boxes x 4 point coordinates (x_min, y_min, x_max, y_max)\n",
    "boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
    "num_of_people = boxes.shape[0]\n",
    "print(str(num_of_people)+' of people has been identified' )\n",
    "\n",
    "img_draw = img.copy()\n",
    "draw = ImageDraw.Draw(img_draw)\n",
    "for i, (box, point) in enumerate(zip(boxes, points)):\n",
    "    draw.rectangle(box.tolist(), width=5) # draw the rectangle of the entire face\n",
    "    for p in point:\n",
    "        draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=0) # draw the rectangle of eyes, nose and mouth corners\n",
    "        extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "        img_draw.save('annotated_faces.png')\n",
    "\n",
    "img_probs  = []\n",
    "for i in range(num_of_people):\n",
    "    img_crop_single = img_cropped[i,...].unsqueeze(0)# Add batch dimension: [channels, height, width] -> [1, channels, height, width]\n",
    "    img_crop_single = img_crop_single.to(device)\n",
    "    img_single_probs = resnet(img_crop_single)\n",
    "    img_single_probs_cpu = img_single_probs.cpu()\n",
    "    img_single_probs_cpu = img_single_probs_cpu.detach().numpy()\n",
    "    img_probs.append(img_single_probs_cpu)\n",
    "\n",
    "# plt.ion()\n",
    "# plt.plot(img_probs[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\initi\\OneDrive\\Documents\\GitHub\\facenet-pytorch\\data\\lfw\\lfw'\n",
    "pairs_path = r'C:\\Users\\initi\\OneDrive\\Documents\\GitHub\\facenet-pytorch\\data\\lfw\\lfwpairs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 为减少GPU内存使用，删除mtcnn\n",
    "del mtcnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 15\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()} # build dict for index:class(people) pairs\n",
    "\n",
    "# %%\n",
    "# 从MTCNN裁剪的图像输出创建数据集和数据加载器\n",
    "\n",
    "embed_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SequentialSampler(dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 加载预训练的Resnet模型\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained='vggface2'\n",
    ").to(device)\n",
    "\n",
    "classes = []\n",
    "embeddings = []\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in embed_loader:\n",
    "        xb = xb.to(device)\n",
    "        b_embeddings = resnet(xb)\n",
    "        b_embeddings = b_embeddings.to('cpu').numpy()\n",
    "        classes.extend(yb.numpy())\n",
    "        embeddings.extend(b_embeddings)\n",
    "\n",
    "# Assuming `embeddings` is a numpy array and `labels` contains the corresponding labels\n",
    "np.save(\"embeddings.npy\", embeddings)\n",
    "np.save(\"classes.npy\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(embeddings1, embeddings2, distance_metric=0):\n",
    "    if distance_metric==0:\n",
    "        # Euclidian distance\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff),1)\n",
    "    elif distance_metric==1:\n",
    "        # 基于余弦相似度的距离\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "    else:\n",
    "        raise 'Undefined distance metric %d' % distance_metric\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold =  1.16 # this is referecn number from lfw_evaluate.py line 201. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min dist is [0.3664642]\n",
      "min dist is [0.30911055]\n",
      "min dist is [0.5099769]\n",
      "min dist is [0.72873294]\n"
     ]
    }
   ],
   "source": [
    "match_people= []\n",
    "for j in range(num_of_people): \n",
    "    match_class = 0\n",
    "    dist_min = float('inf')\n",
    "    img_single_probs = img_probs[j]\n",
    "    for i, train_embed in enumerate(embeddings):\n",
    "        #train_embed = train_embed.numpy()\n",
    "        dist = distance(train_embed, img_single_probs)\n",
    "        if dist < dist_min:\n",
    "            dist_min = dist\n",
    "            match_class = classes[i]\n",
    "    if dist_min <= threshold :       \n",
    "        match_people.append(dataset.idx_to_class[match_class]) \n",
    "    else:\n",
    "        match_people.append(\"Unknown\")\n",
    "    print(\"min dist is \"+ str(dist_min))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LeBron_James', 'Michael_Jordan', 'Unknown', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "print(match_people)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
