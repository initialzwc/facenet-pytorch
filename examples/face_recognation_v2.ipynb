{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1,fixed_image_standardization, training, extract_face\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,SequentialSampler\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image, ImageDraw,ImageFont\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') # change backend enviroment so plot is working \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(embeddings1, embeddings2, distance_metric=0):\n",
    "    if distance_metric==0:\n",
    "        # Euclidian distance\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff),1)\n",
    "    elif distance_metric==1:\n",
    "        # 基于余弦相似度的距离\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "    else:\n",
    "        raise 'Undefined distance metric %d' % distance_metric\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "6 of people has been identified\n"
     ]
    }
   ],
   "source": [
    "# USE MTCNN TO EXTRACT FACES AND RETURN FOLLOWING FFEATURES\n",
    "# -- BONDING BOXES OF EACH PEOPLE'S FACE, AND ITS DIMENSION AND LOCATIONS\n",
    "# -- CONFIDENCE LEVEL THAT THE BONDING BOX IS CORRESPONDING TO A FACE (PROBS)\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Extract the embeddings from people's face only\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device,\n",
    "    keep_all=True, # by setting true, retured img_cropped dimension will be batch_size x identified_people_num x chanels x pixel_x x pixel_y\n",
    ")\n",
    "\n",
    "img = Image.open('C:/Users/initi/OneDrive/Documents/GitHub/facenet-pytorch/data/test_images/test/sombody.jpg')  # Replace with the path to your image file\n",
    "img_cropped = mtcnn(img, save_path=\"C:/Users/initi/OneDrive/Documents/GitHub/facenet-pytorch/data/test_images/test/sombody_cropped.jpg\")\n",
    "\n",
    "# box dim : number_of_people x 4 point coordinates (x_min, y_min, x_max, y_max)\n",
    "# points dim : number_of_people x 5 boxes x 4 point coordinates (x_min, y_min, x_max, y_max)\n",
    "boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
    "num_of_people = boxes.shape[0]\n",
    "print(str(num_of_people)+' of people has been identified' )\n",
    "\n",
    "# COPY OF ORIGINAL IMAGE FOR LATER NAME LABELING USE\n",
    "img_draw = img.copy()\n",
    "draw = ImageDraw.Draw(img_draw)\n",
    "\n",
    "# %%\n",
    "# 为减少GPU内存使用，删除mtcnn\n",
    "del mtcnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\initi\\OneDrive\\Documents\\GitHub\\facenet-pytorch\\data\\lfw\\lfw'\n",
    "pairs_path = r'C:\\Users\\initi\\OneDrive\\Documents\\GitHub\\facenet-pytorch\\data\\lfw\\lfwpairs.txt'\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()} # build dict for index:class(people) pairs\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "resnet.classify = False # If set to be true, the final fully connected layer will be aciviated, the output dimension is depending on the # of classes that used for the trainning. Adding more people to the trainning will change this number\n",
    "\n",
    "threshold = 1.16\n",
    "embeddings = np.load(\"embeddings.npy\") # EMBEDDINGS FROM ALL TRAINNING DATA, CAN BE OBTAINED FROM FACE_RECOGNATION.IPYNB\n",
    "classes = np.load(\"classes.npy\") # CLASSES FROM ALL TRAINNING DATA, CAN BE OBTAINED FROM FACE_RECOGNATION.IPYNB\n",
    "\n",
    "# Optionally, specify a font\n",
    "# Replace \"arial.ttf\" with a path to a valid font file on your system\n",
    "try:\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=20)\n",
    "except IOError:\n",
    "    font = ImageFont.load_default()  # Fallback to default font\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected people is Tom_Hanks, min dist is [0.3645305]\n",
      "detected people is Jack_Nicholson, min dist is [0.6623132]\n",
      "detected people is Harrison_Ford, min dist is [0.30935055]\n",
      "detected people is Rob_Lowe, min dist is [0.7386179]\n",
      "detected people is Tom_Cruise, min dist is [0.2907584]\n",
      "detected people is Edward_Norton, min dist is [0.38294858]\n"
     ]
    }
   ],
   "source": [
    "# ITERATE EACH FACES IN BONDING BOXES, EXTRACT EMBEDDINGS AND COMPARE WITH SAVED EMBEDDINGS FROM TRAINNING DATA\n",
    "\n",
    "img_probs  = [] # store the embedding vectors for each detected people\n",
    "match_people_list= []\n",
    "for i, (box, point) in enumerate(zip(boxes, points)):\n",
    "    draw.rectangle(box.tolist(), width=5) # draw the rectangle of the entire face\n",
    "    for p in point:\n",
    "        draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=0) # draw the rectangle of eyes, nose and mouth corners\n",
    "        extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
    "        #img_draw.save('annotated_faces.png')\n",
    "    img_crop_single = img_cropped[i,...].unsqueeze(0)# Add batch dimension: [channels, height, width] -> [1, channels, height, width]\n",
    "    img_crop_single = img_crop_single.to(device)\n",
    "    img_single_probs = resnet(img_crop_single)\n",
    "    img_single_probs_cpu = img_single_probs.cpu()\n",
    "    img_single_probs_cpu = img_single_probs_cpu.detach().numpy()\n",
    "    img_probs.append(img_single_probs_cpu)\n",
    "\n",
    "    match_class = 0\n",
    "    dist_min = float('inf')\n",
    "    #img_single_probs = img_probs[i]\n",
    "   \n",
    "    match_people = []\n",
    "\n",
    "    for j, train_embed in enumerate(embeddings):\n",
    "        #train_embed = train_embed.numpy()\n",
    "        dist = distance(train_embed, img_single_probs_cpu)\n",
    "        if dist < dist_min:\n",
    "            dist_min = dist\n",
    "            match_class = classes[j]\n",
    "    if dist_min <= threshold:\n",
    "        match_people =  dataset.idx_to_class[match_class]      \n",
    "        match_people_list.append(match_people) \n",
    "    else:\n",
    "        match_people =  \"Unknown\"      \n",
    "        match_people_list.append(match_people)\n",
    "    text_bbox = draw.textbbox((box[0], box[1]), match_people, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "    # Position the text above the rectangle (or below if there's not enough space)\n",
    "    w = box[2]-box[0]\n",
    "    h = box[3]-box[1]\n",
    "    text_x = box[0]\n",
    "    text_y = box[1] - text_height - 5  # Place text 5 pixels above the rectang\n",
    "    if text_y < 0:  # If text goes below the image, place it above the rectangle\n",
    "        text_y = box[1] + h + 5\n",
    "    print(\"detected people is \"+ match_people + \", min dist is \"+ str(dist_min)) \n",
    "    # Add a filled rectangle as a background for text (optional, for better visibility)\n",
    "    draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height], fill=\"black\")\n",
    "    # Add text\n",
    "    draw.text((text_x, text_y), match_people, fill=\"white\", font=font)\n",
    "    img_draw.save('annotated_faces.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
